{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f855a02",
   "metadata": {},
   "source": [
    "# Explore Fantasy Football Data using Pandas (Python Data Analysis Library) Utility Notebook <a id=\"return\"></a>\n",
    "\n",
    "This notebook contains all the functions used in the 01-explore_ff_league_data.ipynb notebook.\n",
    "<br><br/>\n",
    "\n",
    "**Notebook Sections:**\n",
    "1. [Import Packages](#section1)\n",
    "2. [Function for Reading in Data](#section2)\n",
    "3. [Functions for Data Exploration](#section3)\n",
    "4. [Function to Plot Histograms](#section4)\n",
    "5. [Function to Compute Correlations](#section5)\n",
    "6. [Functions to Compute Categorical Relationships](#section6)\n",
    "7. [Functions to Compute Numerical/Categorical Relationships](#section7)\n",
    "8. [Function to Plot Dates](#section8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916b32a1",
   "metadata": {},
   "source": [
    "## Import Packages <a id=\"section1\"></a>\n",
    "\n",
    "[Return to Top](#return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbc9054-b9c5-41c4-ba62-3d6023671315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages to create/manipulate dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# import package to used today's date\n",
    "from datetime import date\n",
    "\n",
    "# import package to calculate categorical relationships\n",
    "import scipy.stats as ss\n",
    "from collections import Counter\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "import warnings\n",
    "\n",
    "# import packages to create visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib import dates as mpl_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf1b60e-a54c-4f22-b20d-6b277fef7613",
   "metadata": {},
   "source": [
    "## Function for Reading in Data <a id=\"section2\"></a>\n",
    "\n",
    "[Return to Top](#return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f5439-f64a-4aab-9063-56bf2445c3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to read in data from flat file, pickle object, or database\n",
    "def read_data(file_path: str = None, db_path: str = None, db_conn: object = None):\n",
    "    \n",
    "    ''' \n",
    "        This function will check for a file name or a database table name.  If a file name is passed then this function will check for 'csv' or 'pkl' in the file name string\n",
    "        and return a pandas dataframe. If the file name string doesn't contain 'csv' or 'pkl' the function will try to open and read the file and throw an exception if unsuccessful.\n",
    "        \n",
    "        If a table name is passed then the function will query the whole table and return a pandas dataframe.\n",
    "        \n",
    "        param file_path: string containing the full file path where data resides\n",
    "        param db_path: string containing the full database path where data resides\n",
    "        param db_conn: database connection object. needed to read data from database\n",
    "        \n",
    "        returns: pandas dataframe\n",
    "    '''\n",
    "    \n",
    "    # check for file path\n",
    "    if file_path:\n",
    "    \n",
    "        # check for csv file\n",
    "        if 'csv' in file_path.lower():\n",
    "            \n",
    "            # read csv\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "        # check for pickle file\n",
    "        elif 'pkl' in file_path.lower():\n",
    "            \n",
    "            # read pickle\n",
    "            df = pd.read_pickle(file_path)\n",
    "           \n",
    "        # if no csv or pickle file then try opening/reading file\n",
    "        else:\n",
    "            \n",
    "            # try opening file with python's read\n",
    "            try:\n",
    "\n",
    "                # open file\n",
    "                with open(file_path) as f:\n",
    "\n",
    "                    # create contents object\n",
    "                    contents = f.read()\n",
    "\n",
    "                    # create pandas dataframe\n",
    "                    df = pd.DataFrame(contents)\n",
    "\n",
    "            except FileNotFoundError:\n",
    "\n",
    "                # print file not found message\n",
    "                msg = \"Sorry, the file \" + filename + \"does not exist.\"\n",
    "                print(msg)\n",
    "          \n",
    "    # check for table name\n",
    "    if db_path:\n",
    "        \n",
    "        # read table data\n",
    "        df = pd.read_sql(f'SELECT * FROM {db_path}', db_conn)\n",
    "    \n",
    "    # initially explore data\n",
    "    display(df.info(verbose = True, show_counts = True))\n",
    "    \n",
    "    # return dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ce7a26",
   "metadata": {},
   "source": [
    "## Functions for Data Exploration <a id=\"section3\"></a>\n",
    "\n",
    "[Return to Top](#return)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bbc6e3-a7f9-4d0a-a14f-59fab79cba62",
   "metadata": {},
   "source": [
    "Create function to fully print a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c24a222-2538-4c1d-9161-54c9f719a33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to define how to print pandas dataframe\n",
    "def print_full(pd_df: object):\n",
    "    \n",
    "    # set pandas print options\n",
    "    with pd.option_context('display.max_rows', 100\n",
    "                          ,'display.max_columns', 500\n",
    "                          ,'display.precision', 3\n",
    "                          ,'display.colheader_justify', 'center'\n",
    "                          ):\n",
    "        \n",
    "        # print pandas dataframe\n",
    "        display(pd_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9b74df-2a52-4f25-9932-bd12bb47dacd",
   "metadata": {},
   "source": [
    "Create function to explore overall data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c5b18-2096-4096-8fd6-7a9e6aef4ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to explore overall data\n",
    "def explore_all_data(data: object\n",
    "                    ,print_flag: bool = None\n",
    "                    ,file_name_flag: bool = None\n",
    "                    ,data_name_var: str = None\n",
    "                    ,output_dir: str = None\n",
    "                    ):\n",
    "    '''\n",
    "    Function to explore overall data\n",
    "    \n",
    "    param data: pandas dataframe\n",
    "          print_flag: boolean flag; determines whether or not to print output within notebook\n",
    "          file_name_flag: boolean flag; determines whether or not to remove last period and all subsequent characters from a string (i.e. '.csv')\n",
    "          data_name_var: string containing data source name.  typically the file name or database table name\n",
    "          output_dir: string containing output data directory\n",
    "    '''\n",
    "    \n",
    "    # calculate row and column counts\n",
    "    row_count, column_count = data.shape[0], data.shape[1]\n",
    "    \n",
    "    # create numerical column list\n",
    "    num_col = [i for i in (data.select_dtypes(include=['float64', 'int64']).columns)]\n",
    "    \n",
    "    # create categorical column list\n",
    "    cat_col = [i for i in (data.select_dtypes(include=['string', 'object', 'category']).columns)]\n",
    "    \n",
    "    # create date column list\n",
    "    date_col = [i for i in (data.select_dtypes(include=['datetime64[ns]']).columns)]\n",
    "    \n",
    "    # create boolean column list\n",
    "    bool_col = [i for i in (data.select_dtypes(include=['boolean']).columns)]\n",
    "    \n",
    "    # create unknown column list\n",
    "    unk_col = list(set(list(data.columns)) - set(num_col) - set(cat_col) - set(date_col) - set(bool_col))\n",
    "    \n",
    "    # calculate percentage null\n",
    "    perc_null = round(100 * data.isnull().sum().sum() / (len(data) * len(list(data.columns))), 2)\n",
    "    \n",
    "    # calculate percentage empty\n",
    "    perc_empty = round(100 * data.eq('').sum().sum() / (len(data) * len(list(data.columns))), 2)\n",
    "    \n",
    "    # calculate percentage zero\n",
    "    perc_zero = round(100 * data[num_col].eq(0).sum().sum() / (len(data) * len(list(data.columns))), 2)\n",
    "    \n",
    "    # calculate percentage negative\n",
    "    perc_neg = round(100 * data[num_col].lt(0).sum().sum() / (len(data) * len(list(data.columns))), 2)\n",
    "    \n",
    "    # calculate percentage positive\n",
    "    perc_pos = round(100 * data[num_col].gt(0).sum().sum() / (len(data) * len(list(data.columns))), 2)\n",
    "    \n",
    "    # create dataframe of data types and their respective counts\n",
    "    data_type_df = data.dtypes.value_counts().reset_index()\n",
    "    \n",
    "    # rename columns\n",
    "    data_type_df.columns = ['Data Type', 'Count']\n",
    "    \n",
    "    # remove index\n",
    "    blank_index = [''] * len(data_type_df)\n",
    "    data_type_df.index = blank_index\n",
    "    \n",
    "    # print output created above or save it to csv\n",
    "    if print_flag:\n",
    "        \n",
    "        # print output\n",
    "        print(f'Number of Rows: {row_count}\\nNumber of Columns: {column_count}')\n",
    "        print(f'\\nNumerical Columns: {num_col}')\n",
    "        print(f'\\nCategorical Columns: {cat_col}')\n",
    "        print(f'\\nDate Columns: {date_col}')\n",
    "        print(f'\\nBoolean Columns: {bool_col}')\n",
    "        print(f'\\nUnknown Columns: {unk_col}')\n",
    "        print(f'\\nTotal Percentage of Null Values: {perc_null}')\n",
    "        print(f'\\nTotal Percentage of Empty Values: {perc_empty}')\n",
    "        print(f'\\nTotal Percentage of Zero Values: {perc_zero}')\n",
    "        print(f'\\nTotal Percentage of Negative Values: {perc_neg}')\n",
    "        print(f'\\nTotal Percentage of Positive Values: {perc_pos}\\n')\n",
    "        print_full(data_type_df)\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        # create dataframe with output created above\n",
    "        df = pd.DataFrame.from_dict({'Row Count': [row_count]\n",
    "                                    ,'Column Count': [column_count]\n",
    "                                    ,'Numerical Columns': [num_col]\n",
    "                                    ,'Categorical Columns': [cat_col]\n",
    "                                    ,'Date Columns': [date_col]\n",
    "                                    ,'Boolean Columns': [bool_col]\n",
    "                                    ,'Unknown Columns': [unk_col]\n",
    "                                    ,'Percentage Null': [perc_null]\n",
    "                                    ,'Percentage Empty': [perc_empty]\n",
    "                                    ,'Percentage Zero': [perc_zero]\n",
    "                                    ,'Percentage Negative': [perc_neg]\n",
    "                                    ,'Percentage Positive': [perc_pos]\n",
    "                                    ,'Data Type Counts': [data_type_df]\n",
    "                                    }).T\n",
    "\n",
    "        # if there's a period in the data_name_var and if it's a flat file then remove the last period and all subsequent characters (example: '.pkl' or '.csv)\n",
    "        file_title = data_name_var.rpartition('.')[0] if '.' in data_name_var and file_name_flag else data_name_var\n",
    "\n",
    "        # save to csv\n",
    "        df.to_csv(output_dir + f'01-{file_title}-overall_data_summary_{date.today()}.csv', header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fc114d-46b4-44b2-9d4b-ba9138489e1c",
   "metadata": {},
   "source": [
    "Create function to run data exploration functions below and then save or print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51d8ea4-62d0-4e35-93ec-5c0916257d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to run the data exploration functions below and save or print the output\n",
    "def run_explore_func(data: object\n",
    "                    ,func: object\n",
    "                    ,print_flag: bool = None\n",
    "                    ,file_name_flag: bool = None\n",
    "                    ,data_name_var: str = None\n",
    "                    ,output_dir: str = None\n",
    "                    ):\n",
    "    \n",
    "    '''\n",
    "    Function to run the data exploration functions created in this notebook\n",
    "    \n",
    "    param data: pandas dataframe\n",
    "          func: data exploration function object \n",
    "          print_flag: boolean flag; determines whether or not to print output within notebook\n",
    "          file_name_flag: boolean flag; determines whether or not to remove last period and all subsequent characters from a string (i.e. '.csv')\n",
    "          data_name_var: string containing data source name.  typically the file name or database table name\n",
    "          output_dir: string containing output data directory\n",
    "    '''\n",
    "    \n",
    "    # run data exploration function\n",
    "    df, freq_df_list, first_values_df_list, last_values_df_list = func(data, 5)\n",
    "\n",
    "    # print output or save it to csv\n",
    "    if print_flag:\n",
    "        \n",
    "        # print data summary\n",
    "        print(\"Summary:\\n\")\n",
    "        print_full(df)\n",
    "        \n",
    "        # print most frequent values for each variable\n",
    "        print(\"\\nMost Frequent n Values:\\n\")\n",
    "        for i in range(0, len(freq_df_list)):\n",
    "            print_full(freq_df_list[i])\n",
    "            \n",
    "        # print first values for each variable\n",
    "        print(\"\\nFirst n Values:\\n\")\n",
    "        for j in range(0, len(first_values_df_list)):\n",
    "            print_full(first_values_df_list[j])\n",
    "            \n",
    "        # print last values for each variable\n",
    "        print(\"\\nLast n Values:\\n\")\n",
    "        for k in range(0, len(last_values_df_list)):\n",
    "            print_full(last_values_df_list[k])\n",
    "            \n",
    "        print('\\n') \n",
    "            \n",
    "    else:\n",
    "        \n",
    "        # create most frequent values, first values, and last values columns\n",
    "        df['most_freq'] = freq_df_list\n",
    "        df['first_values'] = first_values_df_list\n",
    "        df['last_values'] = last_values_df_list\n",
    "        \n",
    "        # if there's a period in the data_name_var and if it's a flat file then remove the last period and all subsequent characters (example: '.pkl' or '.csv)\n",
    "        file_title = data_name_var.rpartition('.')[0] if '.' in data_name_var and file_name_flag else data_name_var\n",
    "        \n",
    "        # determine which function is being used and create variables for file name\n",
    "        if 'num' in func.__name__:\n",
    "            data_explore_type = 'num'\n",
    "            file_num = '02'\n",
    "        else:\n",
    "            data_explore_type = 'cat'\n",
    "            file_num = '03'\n",
    "        \n",
    "        # save data summary to csv\n",
    "        df.to_csv(output_dir + f'{file_num}-{file_title}-{data_explore_type}_data_summary_{date.today()}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da893bf3-6b78-4a39-818a-b014e27a0a80",
   "metadata": {},
   "source": [
    "Create function to explore numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6642c4-b40f-4291-8e30-8e345ba107b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to explore numerical data\n",
    "def explore_num_data(data: object, n: int):\n",
    "    '''\n",
    "    Function to explore numerical data\n",
    "    \n",
    "    param data: pandas dataframe\n",
    "    param n: integer to determine the number of most frequent, first, and last values to return (when sorted) for each numerical variable\n",
    "    \n",
    "    returns: a pandas dataframe which includes row count, number of distinct values, 5-number summary, \n",
    "             mean, standard deviation, sum, percentage null, percentage zero, percentage positive, \n",
    "             and percentage negative for each numerical variable\n",
    "             \n",
    "             lists of pandas dataframes which include the most frequent values, the first n values,\n",
    "             and the last n values for each numerical variable \n",
    "    '''\n",
    "    \n",
    "    # create list of numerical columns\n",
    "    columns = [i for i in (data.select_dtypes(include=['float64', 'int64']).columns)]\n",
    "    \n",
    "    # check if numerical columns exist\n",
    "    if len(columns) == 0:\n",
    "        print('No numerical data.')\n",
    "        return\n",
    "    \n",
    "    # select numerical columns within dataframe\n",
    "    data = data[columns]\n",
    "    \n",
    "    # call describe function to calculate count, mean, std, and 5-number summary\n",
    "    describe_df = data.describe()\n",
    "    \n",
    "    # create list of number of distinct values within each numerical column\n",
    "    dist_num = [len(data[i].dropna().unique()) for i in columns]\n",
    "    \n",
    "    # create list of the sum of all values within each numerical column\n",
    "    total_sum = [data[i].sum() for i in columns]\n",
    "    \n",
    "    # calculate total number of rows\n",
    "    total_count = len(data)\n",
    "    \n",
    "    # calculate percentage null for each numerical column\n",
    "    null_perc = [100.0 * data[i].isna().sum() / total_count for i in columns]\n",
    "    \n",
    "    # calculate percentage zero for each numerical column\n",
    "    zero_perc = [100.0 * len(data.loc[data[i] == 0, i]) / total_count for i in columns]\n",
    "    \n",
    "    # calculate percentage positive for each numerical column\n",
    "    pos_perc = [100.0 * len(data.loc[data[i] > 0, i]) / total_count for i in columns]\n",
    "    \n",
    "    # calculate percentage negative for each numerical column\n",
    "    neg_perc = [100.0 * len(data.loc[data[i] < 0, i]) / total_count for i in columns]\n",
    "    \n",
    "    # create temporary dataframe for statistics created above\n",
    "    temp_df = pd.DataFrame({'dist_num': dist_num\n",
    "                           ,'total_sum': total_sum\n",
    "                           ,'null_perc': null_perc\n",
    "                           ,'zero_perc': zero_perc\n",
    "                           ,'pos_perc': pos_perc\n",
    "                           ,'neg_perc': neg_perc\n",
    "                           }).transpose()\n",
    "    \n",
    "    # set columns\n",
    "    temp_df.columns = columns\n",
    "    \n",
    "    # concatenate describe_df and temp_df\n",
    "    df = pd.concat([describe_df, temp_df], sort = False)\n",
    "    \n",
    "    # set index name\n",
    "    df.index.set_names('summary', inplace = True)\n",
    "    \n",
    "    # transpose dataframe\n",
    "    df = df.transpose()\n",
    "    \n",
    "    # reorder dataframe\n",
    "    df = df[['count'\n",
    "            ,'dist_num'\n",
    "            ,'min'\n",
    "            ,'25%'\n",
    "            ,'50%'\n",
    "            ,'75%'\n",
    "            ,'max'\n",
    "            ,'mean'\n",
    "            ,'std'\n",
    "            ,'total_sum'\n",
    "            ,'null_perc'\n",
    "            ,'zero_perc'\n",
    "            ,'pos_perc'\n",
    "            ,'neg_perc'\n",
    "            ]]\n",
    "    \n",
    "    # rename count column\n",
    "    df.rename(columns={'count':'row_count'}, inplace=True)\n",
    "    \n",
    "    # calculate most frequent values within each numerical column\n",
    "    freq = []\n",
    "    for i in columns:\n",
    "        temp_freq = data.groupby(i).agg(count = (i, 'count')).sort_values('count', ascending = False).reset_index().head(n)\n",
    "        temp_freq['freq'] = temp_freq['count'] / total_count\n",
    "        \n",
    "        # remove index\n",
    "        blank_index = [''] * len(temp_freq)\n",
    "        temp_freq.index = blank_index\n",
    "        freq.append(temp_freq)\n",
    "    \n",
    "    # calculate the first n values within each numerical column when sorted\n",
    "    first_values = []\n",
    "    for j in columns:\n",
    "        temp_first_values = data.groupby(j).agg(count = (j, 'count')).sort_values(j, ascending = True).reset_index().head(n)\n",
    "        temp_first_values['freq'] = temp_first_values['count'] / total_count\n",
    "        \n",
    "        # remove index\n",
    "        blank_index = [''] * len(temp_first_values)\n",
    "        temp_first_values.index = blank_index\n",
    "        first_values.append(temp_first_values)\n",
    "    \n",
    "    # calculate the last n values within each numerical column when sorted\n",
    "    last_values = []\n",
    "    for k in columns:\n",
    "        temp_last_values = data.groupby(k).agg(count = (k, 'count')).sort_values(k, ascending = False).reset_index().head(n)\n",
    "        temp_last_values['freq'] = temp_last_values['count'] / total_count\n",
    "        \n",
    "        # remove index\n",
    "        blank_index = [''] * len(temp_last_values)\n",
    "        temp_last_values.index = blank_index\n",
    "        last_values.append(temp_last_values)\n",
    "    \n",
    "    return df, freq, first_values, last_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0ef0e6-a258-4557-9902-eae0352d66c9",
   "metadata": {},
   "source": [
    "Create function to explore categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd7a258-233e-4543-8541-ba3759771bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to explore categorical data\n",
    "def explore_cat_data(data: object, n: int):\n",
    "    '''\n",
    "    Function to explore categorical data\n",
    "    \n",
    "    param data: pandas dataframe\n",
    "    param n: integer to determine the number of most frequent, first, and last values to return (when sorted) for each categorical variable\n",
    "    \n",
    "    returns: a pandas dataframe which includes minimum string length, maximum string length, row count, \n",
    "             number of distince values, percentage null, and percentage empty for each categorical variable\n",
    "             \n",
    "             lists of pandas dataframes which include the most frequent values, the first n values,\n",
    "             and the last n values for each categorical variable \n",
    "    '''\n",
    "    \n",
    "    # create list of categorical columns\n",
    "    columns = [i for i in (data.select_dtypes(include=['string', 'object', 'category', 'boolean']).columns)]\n",
    "    \n",
    "    # check if categorical columns exist\n",
    "    if len(columns) == 0:\n",
    "        print('No categorical data.')\n",
    "        return\n",
    "    \n",
    "    # select categorical columns within dataframe\n",
    "    data = data[columns]\n",
    "    \n",
    "    # calculate row count\n",
    "    count_rows = [data[i].dropna().count() for i in columns]\n",
    "    \n",
    "    # calculate number of distinct values\n",
    "    dist_num = [data[i].dropna().value_counts().count() for i in columns]\n",
    "    \n",
    "    # calculate total number of rows\n",
    "    total_count = len(data)\n",
    "    \n",
    "    # calculate minimum length for each categorical column\n",
    "    min_length = [data[i].astype(str).str.len().min() for i in columns]\n",
    "    \n",
    "    # calculate maximum length for each categorical column\n",
    "    max_length = [data[i].astype(str).str.len().max() for i in columns]\n",
    "    \n",
    "    # calculate percentage null for each categorical column\n",
    "    null_perc = [100.0 * data[i].isna().sum() / total_count for i in columns]\n",
    "    \n",
    "    # calculate percentage empty for each categorical column\n",
    "    empty_perc = [100.0 * len(data.loc[data[i] == '', i]) / total_count for i in columns]    \n",
    "    \n",
    "    # combine above stats into a pandas dataframe\n",
    "    df = pd.DataFrame({'row_count': count_rows\n",
    "                      ,'dist_num': dist_num\n",
    "                      ,'min_length': min_length\n",
    "                      ,'max_length': max_length\n",
    "                      ,'null_perc': null_perc\n",
    "                      ,'empty_perc': empty_perc\n",
    "                      }).transpose()\n",
    "\t\t\t\t\t  \n",
    "    # add column names\n",
    "    df.columns = columns\n",
    "    \n",
    "    # set index name\n",
    "    df.index.set_names('summary', inplace = True)\n",
    "    \n",
    "    # transpose dataframe\n",
    "    df = df.transpose()\n",
    "    \n",
    "    # calculate most frequent values within each categorical column\n",
    "    freq = []\n",
    "    for i in columns:\n",
    "        temp_freq = data.groupby(i).agg(count = (i, 'count')).sort_values('count', ascending = False).reset_index().head(n)\n",
    "        temp_freq['freq'] = temp_freq['count'] / total_count\n",
    "        \n",
    "        # remove index\n",
    "        blank_index = [''] * len(temp_freq)\n",
    "        temp_freq.index = blank_index\n",
    "        freq.append(temp_freq)\n",
    "    \n",
    "    # calculate the first n values within each categorical column when sorted\n",
    "    first_values = []\n",
    "    for j in columns:\n",
    "        temp_first_values = data.groupby(j).agg(count = (j, 'count')).sort_values(j, ascending = True).reset_index().head(n)\n",
    "        temp_first_values['freq'] = temp_first_values['count'] / total_count\n",
    "        \n",
    "        # remove index\n",
    "        blank_index = [''] * len(temp_first_values)\n",
    "        temp_first_values.index = blank_index\n",
    "        first_values.append(temp_first_values)\n",
    "    \n",
    "    # calculate the last n values within each categorical column when sorted\n",
    "    last_values = []\n",
    "    for k in columns:\n",
    "        temp_last_values = data.groupby(k).agg(count = (k, 'count')).sort_values(k, ascending = False).reset_index().head(n)\n",
    "        temp_last_values['freq'] = temp_last_values['count'] / total_count\n",
    "        \n",
    "        # remove index\n",
    "        blank_index = [''] * len(temp_last_values)\n",
    "        temp_last_values.index = blank_index\n",
    "        last_values.append(temp_last_values)\n",
    "    \n",
    "    return df, freq, first_values, last_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcce570f",
   "metadata": {},
   "source": [
    "## Function to Plot Histograms <a id=\"section4\"></a>\n",
    "\n",
    "[Return to Top](#return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289b0c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to plot histograms for each numerical variable\n",
    "def plot_hist(data: object\n",
    "             ,print_flag: bool = None\n",
    "             ,file_name_flag: bool = None\n",
    "             ,data_name_var: str = None\n",
    "             ,output_dir: str = None\n",
    "             ):\n",
    "    \n",
    "    '''\n",
    "    Function to plot histogram(s)\n",
    "    \n",
    "    param data: pandas dataframe\n",
    "          print_flag: boolean flag; determines whether or not to print output within notebook\n",
    "          file_name_flag: boolean flag; determines whether or not to remove last period and all subsequent characters from a string (i.e. '.csv')\n",
    "          data_name_var: string containing data source name.  typically the file name or database table name\n",
    "          output_dir: string containing output data directory\n",
    "    '''\n",
    "    \n",
    "    # create list of numerical columns\n",
    "    columns = [i for i in (data.select_dtypes(include=['float64', 'int64']).columns)]\n",
    "    \n",
    "    # check if numerical columns exist\n",
    "    if len(columns) == 0:\n",
    "        print('No numerical data.')\n",
    "        return\n",
    "    \n",
    "    # check for print flag\n",
    "    if print_flag:\n",
    "        \n",
    "        # create histogram for each numerical variable\n",
    "        for col in columns:\n",
    "            f = plt.figure()\n",
    "            plt.hist(data[f'{col}'].dropna(), bins = 50, color = 'green')\n",
    "            plt.title(f'{col}')\n",
    "            plt.show()\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        # if there's a period in the data_name_var and if it's a flat file then remove the last period and all subsequent characters (example: '.pkl' or '.csv)\n",
    "        file_title = data_name_var.rpartition('.')[0] if '.' in data_name_var and file_name_flag else data_name_var\n",
    "        \n",
    "        # create pdf object\n",
    "        pdf_obj = PdfPages(output_dir + f'02-{file_title}-histograms_{date.today()}.pdf')\n",
    "\n",
    "        # create histogram for each numerical variable\n",
    "        for col in columns:\n",
    "            f = plt.figure()\n",
    "            plt.hist(data[f'{col}'].dropna(), bins = 50, color = 'green')\n",
    "            plt.title(f'{col}')\n",
    "            pdf_obj.savefig(f)\n",
    "            plt.close()\n",
    "        pdf_obj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bdfb3a",
   "metadata": {},
   "source": [
    "## Function to Compute Correlations <a id=\"section5\"></a>\n",
    "\n",
    "[Return to Top](#return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfe82f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to plot correlation matrix\n",
    "def corr_matrix(data: object\n",
    "               ,print_flag: bool = None\n",
    "               ,file_name_flag: bool = None\n",
    "               ,data_name_var: str = None\n",
    "               ,output_dir: str = None\n",
    "               ):\n",
    "    \n",
    "    '''\n",
    "    Function to plot correlation matrix\n",
    "    \n",
    "    param data: pandas dataframe\n",
    "          print_flag: boolean flag; determines whether or not to print output within notebook\n",
    "          file_name_flag: boolean flag; determines whether or not to remove last period and all subsequent characters from a string (i.e. '.csv')\n",
    "          data_name_var: string containing data source name.  typically the file name or database table name\n",
    "          output_dir: string containing output data directory\n",
    "    '''\n",
    "    \n",
    "    # create list of numerical columns\n",
    "    columns = list(data.select_dtypes(include=['float64', 'int64']).columns)\n",
    "    \n",
    "    # check if numerical columns exist\n",
    "    if len(columns) == 0:\n",
    "        print('No numerical data.')\n",
    "        return\n",
    "    \n",
    "    # create a correlation matrix for all numerical columns\n",
    "    corr = data.corr()    \n",
    "    \n",
    "    # check for print flag and display heatmap if the columns are less than or equal 15, otherwise, the heatmap is too big to easily read within notebook\n",
    "    if print_flag :\n",
    "        \n",
    "        # create heatmap\n",
    "        fig, ax = plt.subplots(figsize = (8, 8))\n",
    "        g = sns.heatmap(corr, annot = True, fmt = '.2f', cmap = plt.get_cmap('coolwarm'), cbar = False, ax = ax)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=45)\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # if there's a period in the data_name_var and if it's a flat file then remove the last period and all subsequent characters (example: '.pkl' or '.csv)\n",
    "        file_title = data_name_var.rpartition('.')[0] if '.' in data_name_var and file_name_flag else data_name_var\n",
    "        \n",
    "        if len(columns) <= 15:\n",
    "\n",
    "            # create a correlation matrix with a heatmap and export to pdf\n",
    "            fig, ax = plt.subplots(figsize = (8, 8))\n",
    "            g = sns.heatmap(corr, annot = True, fmt = '.2f', cmap = plt.get_cmap('coolwarm'), cbar = False, ax = ax) #, linewidths=0.1, linecolor='gray')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.yticks(rotation=45)\n",
    "            plt.savefig(output_dir + f'02-{file_title}-correlation_analysis_{date.today()}.pdf') # , bbox_inches='tight', pad_inches=0.0)\n",
    "            plt.close(fig)  \n",
    "            \n",
    "        else:\n",
    "            print('Too many variables to effectively visualize plot with a notebook.')\n",
    "          \n",
    "        # save to cvs\n",
    "        corr.to_csv(output_dir + f'02-{file_title}-correlation_analysis_{date.today()}.csv', index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f876e218-05b1-49c7-951b-36e115b1a861",
   "metadata": {},
   "source": [
    "## Functions to Compute Categorical Relationships <a id=\"section6\"></a>\n",
    "\n",
    "[Return to Top](#return)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fd11f6-140a-44aa-bc81-f9282336e4e4",
   "metadata": {},
   "source": [
    "Create function to run the categorical relationships functions below and then save or print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726459f9-8f1b-4451-940b-5b33f723f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to run the categorical relationships functions below\n",
    "def run_cat_rel_func(data: object\n",
    "                    ,print_flag: bool = None\n",
    "                    ,file_name_flag: bool = None\n",
    "                    ,data_name_var: str = None\n",
    "                    ,output_dir: str = None\n",
    "                    ):\n",
    "    \n",
    "    '''\n",
    "    Function to run the categorical relationships functions created in this notebook\n",
    "    \n",
    "    param data: pandas dataframe\n",
    "          print_flag: boolean flag; determines whether or not to print output within notebook\n",
    "          file_name_flag: boolean flag; determines whether or not to remove last period and all subsequent characters from a string (i.e. '.csv')\n",
    "          data_name_var: string containing data source name.  typically the file name or database table name\n",
    "          output_dir: string containing output data directory\n",
    "    '''\n",
    "    \n",
    "    # create list of categorical columns\n",
    "    columns = [i for i in (data.select_dtypes(include=['string', 'object', 'category', 'boolean']).columns)]\n",
    "    \n",
    "    # check number of categorical columns\n",
    "    if len(columns) == 0:\n",
    "        print('No categorical data.')\n",
    "        return\n",
    "    \n",
    "    if len(columns) > 15:\n",
    "        print('Too many variables to efficiently the run categorical relationships functions.')\n",
    "        return\n",
    "    \n",
    "    # create empty list to store all results of the categorical relationships functions\n",
    "    results = []\n",
    "    \n",
    "    # create dataframe to store Cramer's V results in order to create heatmap\n",
    "    cramers_df = pd.DataFrame(np.zeros((len(columns), len(columns))), columns = columns, index = columns)\n",
    "    \n",
    "    # create dataframe to store Theil's U results in order to create heatmap\n",
    "    theils_df = pd.DataFrame(np.zeros((len(columns), len(columns))), columns = columns, index = columns)\n",
    "    \n",
    "    # for each unique categorical column pair run the Chi-Squared, Cramer's V, and Theil's U statistical tests\n",
    "    for column_pair in itertools.combinations(columns, 2):\n",
    "        \n",
    "        # run chi-squared function\n",
    "        test_stat, p_value, dof, crosstab, expected_df, assumption_flag, dependent_var = chi_squared(data[column_pair[0]], data[column_pair[1]])\n",
    "        \n",
    "        # create chi-squared result variable\n",
    "        chi_result = 'Dependent (reject H0)' if dependent_var and assumption_flag else 'Independent (fail to reject H0)' if assumption_flag else 'Chi-Squared Assumptions Not Met'\n",
    "        \n",
    "        # run Cramer's V function\n",
    "        cramers = cramers_v(data[column_pair[0]], data[column_pair[1]])\n",
    "        \n",
    "        # add Cramer's V value to cramers_df\n",
    "        cramers_df.at[column_pair[0], column_pair[1]] = cramers\n",
    "        cramers_df.at[column_pair[1], column_pair[0]] = cramers\n",
    "        \n",
    "        # adding 1.00 to the diagonal since each variable is 100% associated with itself\n",
    "        cramers_df.at[column_pair[0], column_pair[0]] = 1.00\n",
    "        cramers_df.at[column_pair[1], column_pair[1]] = 1.00\n",
    "        \n",
    "        # run Theil's U function\n",
    "        theils = theils_u(data[column_pair[0]], data[column_pair[1]])\n",
    "        \n",
    "        # reverse column pairs and run Theil's U function due to the test's asymmetrical nature\n",
    "        rev_theils = theils_u(data[column_pair[1]], data[column_pair[0]])\n",
    "        \n",
    "        # add Theil's U value to theils_df\n",
    "        theils_df.at[column_pair[0], column_pair[1]] = theils\n",
    "        theils_df.at[column_pair[1], column_pair[0]] = rev_theils\n",
    "        \n",
    "        # adding 1.00 to the diagonal since each variable is 100% associated with itself\n",
    "        theils_df.at[column_pair[0], column_pair[0]] = 1.00\n",
    "        theils_df.at[column_pair[1], column_pair[1]] = 1.00\n",
    "        \n",
    "        # append results of the categorical relationships functions\n",
    "        results.append((column_pair[0], column_pair[1], test_stat, p_value, dof, chi_result, cramers, theils, rev_theils))\n",
    "        \n",
    "    # create dataframe\n",
    "    df = pd.DataFrame(results, columns = ['x1', 'x2', 'test_stat', 'p_value', 'dof', 'chi_result', 'cramers', 'theils (x2 predicts x1)', 'rev_theils (x1 predicts x2)'])\n",
    "    \n",
    "    # check for print flag\n",
    "    if print_flag:\n",
    "        \n",
    "        # print dataframe\n",
    "        print_full(df)\n",
    "        print('\\n')\n",
    "        \n",
    "        # create Cramer's V heatmap\n",
    "        fig, ax = plt.subplots(figsize = (8, 8))\n",
    "        g = sns.heatmap(cramers_df, annot = True, fmt = '.2f', cmap = plt.get_cmap('coolwarm'), cbar = False, ax = ax)\n",
    "        ax.set_title(\"Cramér's V\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=45)\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "        \n",
    "        # create Theil's U heatmap\n",
    "        fig, ax = plt.subplots(figsize = (8, 8))\n",
    "        g = sns.heatmap(theils_df, annot = True, fmt = '.2f', cmap = plt.get_cmap('coolwarm'), cbar = False, ax = ax)\n",
    "        ax.set_title(\"Theil's U\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=45)\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # if there's a period in the data_name_var and if it's a flat file then remove the last period and all subsequent characters (example: '.pkl' or '.csv)\n",
    "        file_title = data_name_var.rpartition('.')[0] if '.' in data_name_var and file_name_flag else data_name_var\n",
    "\n",
    "        # save to csv\n",
    "        df.to_csv(output_dir + f'03-{file_title}-categorical_relationships_{date.today()}.csv', index = False)\n",
    "        \n",
    "        # create a  heatmap and export to pdf\n",
    "        fig, ax = plt.subplots(figsize = (8, 8))\n",
    "        g = sns.heatmap(cramers_df, annot = True, fmt = '.2f', cmap = plt.get_cmap('coolwarm'), cbar = False, ax = ax)\n",
    "        ax.set_title(\"Cramér's V\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=45)\n",
    "        plt.savefig(output_dir + f'03-{file_title}-cramers_v_{date.today()}.pdf', bbox_inches='tight', pad_inches=0.0)\n",
    "        plt.close(fig)  \n",
    "           \n",
    "        # create a heatmap and export to pdf\n",
    "        fig, ax = plt.subplots(figsize = (8, 8))\n",
    "        g = sns.heatmap(theils_df, annot = True, fmt = '.2f', cmap = plt.get_cmap('coolwarm'), cbar = False, ax = ax)\n",
    "        ax.set_title(\"Theil's U\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=45)\n",
    "        plt.savefig(output_dir + f'03-{file_title}-theils_u_{date.today()}.pdf', bbox_inches='tight', pad_inches=0.0)\n",
    "        plt.close(fig)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b2b279-ffa0-460e-83d8-b502fba08c57",
   "metadata": {},
   "source": [
    "Create function to compute Chi-Squared Test of Independence.\n",
    "\n",
    "**Sources:** \n",
    "* https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9\n",
    "* https://github.com/shakedzy/dython/blob/master/dython/nominal.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd5fb04-f8a3-461d-9627-68cc3b44cada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to compute chi-squared test of independence\n",
    "def chi_squared(x: object, y: object):\n",
    "    \n",
    "    '''\n",
    "    Function to calculate chi-squared test of independence using both SciPy's chi2_contingency() and pandas’s crosstab()\n",
    "    \n",
    "    The null hypothesis (H0) and alternative hypothesis (H1) of the Chi-Square Test of Independence can be expressed in two different but equivalent ways:\n",
    "\n",
    "    1. H0: \"[Variable 1] is independent of [Variable 2]\"\n",
    "    2. H1: \"[Variable 1] is not independent of [Variable 2]\"\n",
    "\n",
    "    OR\n",
    "\n",
    "    1. H0: \"[Variable 1] is not associated with [Variable 2]\"\n",
    "    2. H1: \"[Variable 1] is associated with [Variable 2]\"\n",
    "    \n",
    "    param x: list / NumPy ndarray / Pandas Series / A sequence of measurements\n",
    "          y: list / NumPy ndarray / Pandas Series / A sequence of measurements\n",
    "          \n",
    "    returns test statistic: float\n",
    "            p-value: float\n",
    "            degrees of freedom: float\n",
    "            expected counts: dataframe\n",
    "            chi-squared assumptions flag: boolean\n",
    "            dependent variable flag: boolean\n",
    "    '''\n",
    "    \n",
    "    # create cross tabulation dataframe\n",
    "    crosstab = pd.crosstab(x, y)\n",
    "    \n",
    "    # run chi-squared test\n",
    "    test_stat, p_value, dof, expected_arr = chi2_contingency(crosstab)\n",
    "    \n",
    "    # create dataframe from the arrays of expected frequencies\n",
    "    expected_df = pd.DataFrame(expected_arr, index = crosstab.index, columns = crosstab.columns)\n",
    "    \n",
    "    # calculate number of individual expected counts that are less than 1\n",
    "    exp_cnts_1 = len([value for array in expected_arr for value in array if value < 1])\n",
    "    \n",
    "    # calculate percentage of individual expected counts that are less than 5\n",
    "    exp_perc_5 = len([value for array in expected_arr for value in array if value < 5]) / len([value for array in expected_arr for value in array])\n",
    "    \n",
    "    # check chi-squared assumption (all individual expected counts are 1 or greater and no more than 20% of expected counts are less than 5)\n",
    "    if exp_cnts_1 > 0 and exp_perc_5 > 0.2:\n",
    "        \n",
    "        # create chi-squared assumptions flag (True = assumptions met; False = assumptions not met)\n",
    "        assumption_flag = False\n",
    "        \n",
    "        # return chi-squared test results and chi-squared assumptions flag\n",
    "        return test_stat, p_value, dof, crosstab, expected_df, assumption_flag, None\n",
    "    \n",
    "    # check chi-squared assumption (cross tabulation table is at least 2x2)\n",
    "    elif len(crosstab.columns) < 2 and len(crosstab.index) < 2:\n",
    "        \n",
    "        # create chi-squared assumptions flag (True = assumptions met; False = assumptions not met)\n",
    "        assumption_flag = False\n",
    "        \n",
    "        # return chi-squared test results and chi-squared assumptions flag\n",
    "        return test_stat, p_value, dof, crosstab, expected_df, assumption_flag, None\n",
    "    \n",
    "    # if the assumptions above are met then interpret test-statistic and p-value\n",
    "    else:\n",
    "        \n",
    "        # create chi-squared assumptions flag (True = assumptions met; False = assumptions not met)\n",
    "        assumption_flag = True\n",
    "        \n",
    "        # set probability of 95%\n",
    "        prob = 0.95\n",
    "        \n",
    "        # calculate critical value\n",
    "        critical = chi2.ppf(prob, dof)\n",
    "        \n",
    "        # calculate alpha value\n",
    "        alpha = 1.0 - prob\n",
    "        \n",
    "        # interpret test-statistic and p-value\n",
    "        if abs(test_stat) >= critical or p <= alpha:\n",
    "            \n",
    "            # set dependent variable flag (True = dependent (reject H0); False = independent (fail to reject H0))\n",
    "            dependent_var = True\n",
    "            \n",
    "            # return chi-squared test results, chi-squared assumptions flag, and dependent variable flag\n",
    "            return test_stat, p_value, dof, crosstab, expected_df, assumption_flag, dependent_var\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # set dependent variable flag\n",
    "            dependent_var = False\n",
    "            \n",
    "            # return chi-squared test results, chi-squared assumptions flag, and dependent variable flag\n",
    "            return test_stat, p_value, dof, crosstab, expected_df, assumption_flag, dependent_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd0c9ff-d6e7-48e9-b236-868a2e1698d0",
   "metadata": {},
   "source": [
    "Create functions to drop or replace null values.\n",
    "\n",
    "**Sources:** \n",
    "* https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9\n",
    "* https://github.com/shakedzy/dython/blob/master/dython/nominal.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971daf5d-79f1-4a7f-96c7-2be04474915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to drop null values\n",
    "def remove_incomplete_samples(x: object, y: object):\n",
    "    \n",
    "    # replace None with numpy's nan value\n",
    "    x = [v if v is not None else np.nan for v in x]\n",
    "    y = [v if v is not None else np.nan for v in y]\n",
    "    \n",
    "    # create numpy array\n",
    "    arr = np.array([x, y]).transpose()\n",
    "    \n",
    "    # remove nan values\n",
    "    arr = arr[~np.isnan(arr).any(axis=1)].transpose()\n",
    "    \n",
    "    # if x is a list then return numpy arrays as list; else return arrays\n",
    "    if isinstance(x, list):\n",
    "        return arr[0].tolist(), arr[1].tolist()\n",
    "    else:\n",
    "        return arr[0], arr[1]\n",
    "\n",
    "# create function to replace null values with value n\n",
    "def replace_nan_with_value(x: object, y: object, value: int):\n",
    "    \n",
    "    # replace null values with value n\n",
    "    x = np.array([v if v == v and v is not None else value for v in x])  # NaN != NaN\n",
    "    \n",
    "    # replace null values with value n\n",
    "    y = np.array([v if v == v and v is not None else value for v in y])\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcbc756-47ac-47fc-93c9-2bef163298ed",
   "metadata": {},
   "source": [
    "Create function to compute Cramer's V.\n",
    "\n",
    "**Sources:** \n",
    "* https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9\n",
    "* https://github.com/shakedzy/dython/blob/master/dython/nominal.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b8cf8f-d3bd-47ac-b353-4ddac7b1043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to calculate Cramer's V\n",
    "def cramers_v(x: object,\n",
    "              y: object,\n",
    "              bias_correction=True,\n",
    "              nan_strategy = 'replace',\n",
    "              nan_replace_value = 0.0):\n",
    "    \"\"\"\n",
    "    Calculates Cramer's V statistic for categorical-categorical association.\n",
    "    This is a symmetric coefficient: V(x,y) = V(y,x)\n",
    "    Original function taken from: https://stackoverflow.com/a/46498792/5863503\n",
    "    Wikipedia: https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : list / NumPy ndarray / Pandas Series / A sequence of categorical measurements\n",
    "    y : list / NumPy ndarray / Pandas Series / A sequence of categorical measurements\n",
    "    bias_correction : Boolean, default = True\n",
    "        Use bias correction from Bergsma and Wicher,\n",
    "        Journal of the Korean Statistical Society 42 (2013): 323-328.\n",
    "    nan_strategy : string, default = 'replace'\n",
    "        How to handle missing values: can be either 'drop' to remove samples\n",
    "        with missing values, or 'replace' to replace all missing values with\n",
    "        the nan_replace_value. Missing values are None and np.nan.\n",
    "    nan_replace_value : any, default = 0.0\n",
    "        The value used to replace missing values with. Only applicable when\n",
    "        nan_strategy is set to 'replace'.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float in the range of [0,1]\n",
    "    \"\"\"\n",
    "    \n",
    "    # check for null handeling strategy\n",
    "    if nan_strategy == 'replace':\n",
    "        \n",
    "        # run replace nulls function\n",
    "        x, y = replace_nan_with_value(x, y, nan_replace_value)\n",
    "        \n",
    "    elif nan_strategy == 'drop':\n",
    "        \n",
    "        # run remove nulls function\n",
    "        x, y = remove_incomplete_samples(x, y)\n",
    "        \n",
    "    # create cross tabulation dataframe\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    \n",
    "    # compute chi-squared test statistic\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    \n",
    "    # calculate matrix size\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    \n",
    "    # calculate phi\n",
    "    phi2 = chi2 / n\n",
    "    \n",
    "    # calculate number of rows and columns\n",
    "    r, k = confusion_matrix.shape\n",
    "    \n",
    "    # check for bias correction\n",
    "    if bias_correction:\n",
    "        \n",
    "        # calculate Cramer's V using bias correction\n",
    "        phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
    "        rcorr = r - ((r - 1) ** 2) / (n - 1)\n",
    "        kcorr = k - ((k - 1) ** 2) / (n - 1)\n",
    "        if min((kcorr - 1), (rcorr - 1)) == 0:\n",
    "            warnings.warn(\n",
    "                \"Unable to calculate Cramer's V using bias correction. Consider using bias_correction=False\",\n",
    "                RuntimeWarning)\n",
    "            return np.nan\n",
    "        else:\n",
    "            v = np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))\n",
    "    else:\n",
    "        \n",
    "        # calculate Cramer's V without bias correction\n",
    "        v = np.sqrt(phi2 / min(k - 1, r - 1))\n",
    "        \n",
    "    # check if Cramer's V is negative or greater than 1\n",
    "    if -1e-13 <= v < 0. or 1. < v <= 1. + 1e-13:\n",
    "        \n",
    "        # round Cramer's V\n",
    "        rounded_v = 0. if v < 0 else 1.\n",
    "        \n",
    "        # print warning and return rounded Cramer's V\n",
    "        warnings.warn(f'Rounded V = {v} to {rounded_v}. This is probably due to floating point precision issues.', RuntimeWarning)\n",
    "        return rounded_v\n",
    "    else:\n",
    "        \n",
    "        # return Cramer's V\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46749ba8-de21-47be-af4b-3d60078e3178",
   "metadata": {},
   "source": [
    "Create function to compute Conditional Entropy.\n",
    "\n",
    "**Sources:** \n",
    "* https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9\n",
    "* https://github.com/shakedzy/dython/blob/master/dython/nominal.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b04ddfd-5dd8-459a-b56f-844cc8ea356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to calculate conditional entropy\n",
    "def conditional_entropy(x: object,\n",
    "                        y: object,\n",
    "                        nan_strategy = 'replace',\n",
    "                        nan_replace_value=0.0,\n",
    "                        log_base: float = math.e):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates the conditional entropy of x given y: S(x|y)\n",
    "    Wikipedia: https://en.wikipedia.org/wiki/Conditional_entropy\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : list / NumPy ndarray / Pandas Series / A sequence of measurements\n",
    "    y : list / NumPy ndarray / Pandas Series / A sequence of measurements\n",
    "    nan_strategy : string, default = 'replace'\n",
    "        How to handle missing values: can be either 'drop' to remove samples\n",
    "        with missing values, or 'replace' to replace all missing values with\n",
    "        the nan_replace_value. Missing values are None and np.nan.\n",
    "    nan_replace_value : any, default = 0.0\n",
    "        The value used to replace missing values with. Only applicable when\n",
    "        nan_strategy is set to 'replace'.\n",
    "    log_base: float, default = e\n",
    "        specifying base for calculating entropy. Default is base e.\n",
    "        \n",
    "    Returns: \n",
    "    --------\n",
    "    float\n",
    "    \"\"\"\n",
    "    \n",
    "    # check for null handeling strategy\n",
    "    if nan_strategy == 'replace':\n",
    "        \n",
    "        # run replace nulls function\n",
    "        x, y = replace_nan_with_value(x, y, nan_replace_value)\n",
    "        \n",
    "    elif nan_strategy == 'drop':\n",
    "        \n",
    "        # run remove nulls function\n",
    "        x, y = remove_incomplete_samples(x, y)\n",
    "        \n",
    "    # create dictionary where the key is a class in a categorical column (y) and the value is the count of that class\n",
    "    y_counter = Counter(y)\n",
    "    \n",
    "    # create Counter dict using paied classes from both x and y \n",
    "    xy_counter = Counter(list(zip(x, y)))\n",
    "    total_occurrences = sum(y_counter.values())\n",
    "    entropy = 0.0\n",
    "    \n",
    "    # calculate conditional entropy\n",
    "    for xy in xy_counter.keys():\n",
    "        p_xy = xy_counter[xy] / total_occurrences\n",
    "        p_y = y_counter[xy[1]] / total_occurrences\n",
    "        entropy += p_xy * math.log(p_y / p_xy, log_base)\n",
    "        \n",
    "    # return conditional entropy\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39ff9e3-4204-4803-97d0-4da7575ee54b",
   "metadata": {},
   "source": [
    "Create function to compute Theil's U.\n",
    "\n",
    "**Sources:** \n",
    "* https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9\n",
    "* https://github.com/shakedzy/dython/blob/master/dython/nominal.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4981a62-41c4-4858-9f0f-7d1271a878ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to compute Theil's U\n",
    "def theils_u(x: object,\n",
    "             y: object,\n",
    "             nan_strategy = 'replace',\n",
    "             nan_replace_value = 0.0):\n",
    "    \n",
    "    '''\n",
    "    Calculates Theil's U statistic (Uncertainty coefficient) for categorical-\n",
    "    categorical association. This is the uncertainty of x given y: value is\n",
    "    on the range of [0,1] - where 0 means y provides no information about\n",
    "    x, and 1 means y provides full information about x.\n",
    "    This is an asymmetric coefficient: U(x,y) != U(y,x)\n",
    "    Wikipedia: https://en.wikipedia.org/wiki/Uncertainty_coefficient\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : list / NumPy ndarray / Pandas Series / A sequence of categorical measurements\n",
    "    y : list / NumPy ndarray / Pandas Series / A sequence of categorical measurements\n",
    "    nan_strategy : string, default = 'replace'\n",
    "        How to handle missing values: can be either 'drop' to remove samples\n",
    "        with missing values, or 'replace' to replace all missing values with\n",
    "        the nan_replace_value. Missing values are None and np.nan.\n",
    "    nan_replace_value : any, default = 0.0\n",
    "        The value used to replace missing values with. Only applicable when\n",
    "        nan_strategy is set to 'replace'.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float in the range of [0,1]\n",
    "    '''\n",
    "    \n",
    "    # check for null handeling strategy\n",
    "    if nan_strategy == 'replace':\n",
    "        \n",
    "        # run replace nulls function\n",
    "        x, y = replace_nan_with_value(x, y, nan_replace_value)\n",
    "        \n",
    "    elif nan_strategy == 'drop':\n",
    "        \n",
    "        # run remove nulls function\n",
    "        x, y = remove_incomplete_samples(x, y)\n",
    "        \n",
    "    # run  conditional entropy function\n",
    "    s_xy = conditional_entropy(x, y)\n",
    "    \n",
    "    # create Counter dict using x\n",
    "    x_counter = Counter(x)\n",
    "    \n",
    "    # sum all counts from x_counter\n",
    "    total_occurrences = sum(x_counter.values())\n",
    "    \n",
    "    # divide each count value by total occurrences\n",
    "    p_x = list(map(lambda n: n / total_occurrences, x_counter.values()))\n",
    "    \n",
    "    # calculate entropy\n",
    "    s_x = ss.entropy(p_x)\n",
    "    \n",
    "    # return 1 if entropy if 0\n",
    "    if s_x == 0:\n",
    "        return 1.\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # calculate Theil's U\n",
    "        u = (s_x - s_xy) / s_x\n",
    "        \n",
    "        # check if Theil's U is negative or greater than 1\n",
    "        if -1e-13 <= u < 0. or 1. < u <= 1.+1e-13:\n",
    "            \n",
    "            # round Theil's U\n",
    "            rounded_u = 0. if u < 0 else 1.\n",
    "            \n",
    "            # print warning and return rounded Theil's U\n",
    "            warnings.warn(f'Rounded U = {u} to {rounded_u}. This is probably due to floating point precision issues.',RuntimeWarning)\n",
    "            return rounded_u\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # return Theil's U\n",
    "            return u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3059da58-d8c4-4162-9f6e-b0f76563d0e7",
   "metadata": {},
   "source": [
    "## Function to Compute Numerical/Categorical Relationships <a id=\"section7\"></a>\n",
    "\n",
    "**Sources:** \n",
    "* https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9\n",
    "* https://github.com/shakedzy/dython/blob/master/dython/nominal.py\n",
    "\n",
    "[Return to Top](#return)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa88c49-b28a-461c-852b-aa13a521dcb6",
   "metadata": {},
   "source": [
    "Create function to run  correlation ratio function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906ea046-e0cd-44e7-b2e5-5371b9ed9b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to run the correlation ratio function below\n",
    "def run_corr_ratio_func(data: object\n",
    "                       ,print_flag: bool = None\n",
    "                       ,file_name_flag: bool = None\n",
    "                       ,data_name_var: str = None\n",
    "                       ,output_dir: str = None\n",
    "                       ):\n",
    "    \n",
    "    '''\n",
    "    Function to run the correlation ratio function created in this notebook\n",
    "    \n",
    "    param data: pandas dataframe\n",
    "          print_flag: boolean flag; determines whether or not to print output within notebook\n",
    "          file_name_flag: boolean flag; determines whether or not to remove last period and all subsequent characters from a string (i.e. '.csv')\n",
    "          data_name_var: string containing data source name.  typically the file name or database table name\n",
    "          output_dir: string containing output data directory\n",
    "    '''\n",
    "    \n",
    "    # create list of categorical columns    \n",
    "    cat_col = [i for i in (data.select_dtypes(include=['string', 'object', 'category', 'boolean']).columns)]\n",
    "\n",
    "    # create numerical column list\n",
    "    num_col = [i for i in (data.select_dtypes(include=['float64', 'int64']).columns)]\n",
    "    \n",
    "    # check if categorical and numerical columns exist\n",
    "    if len(cat_col) == 0 or len(num_col) == 0:\n",
    "        print('Need both categorical and numerical data to compute correlation ratio.')\n",
    "        return\n",
    "    \n",
    "    # create empty list to store results\n",
    "    results = []\n",
    "    \n",
    "    # for each unique numerical and categorical column pair run the correlation ratio function\n",
    "    for column_pair in itertools.product(cat_col, num_col):\n",
    "        \n",
    "        # run tcorrelation ratio function\n",
    "        corr_ratio = correlation_ratio(data[column_pair[0]], data[column_pair[1]])\n",
    "        \n",
    "        # append results of the categorical relationships functions\n",
    "        results.append((column_pair[0], column_pair[1], corr_ratio))\n",
    "        \n",
    "    # create dataframe\n",
    "    df = pd.DataFrame(results, columns = ['x1', 'x2', 'corr_ratio'])\n",
    "    \n",
    "    # check for print flag\n",
    "    if print_flag:\n",
    "        \n",
    "        # print dataframe\n",
    "        print_full(df)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # if there's a period in the data_name_var and if it's a flat file then remove the last period and all subsequent characters (example: '.pkl' or '.csv)\n",
    "        file_title = data_name_var.rpartition('.')[0] if '.' in data_name_var and file_name_flag else data_name_var\n",
    "\n",
    "        # save to csv\n",
    "        df.to_csv(output_dir + f'04-{file_title}-cat_num_relationships_{date.today()}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966b16d6-19e3-495d-95bf-8bc1a7f5365e",
   "metadata": {},
   "source": [
    "Create function to convert data into useable format for the correlation ratio function.\n",
    "\n",
    "**Sources:** \n",
    "* https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9\n",
    "* https://github.com/shakedzy/dython/blob/master/dython/nominal.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3337714-6ba1-4de5-83d1-7e494c49c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to convert data into useable format for the correlation_ratio ratio\n",
    "def convert(data: object, to: str, copy: bool = True):\n",
    "    \n",
    "    # set converted to None\n",
    "    converted = None\n",
    "    \n",
    "    # check if to string is 'array'\n",
    "    if to == 'array':\n",
    "        \n",
    "        # check for type of array and convert data into useable format for the correlation_ratio ratio \n",
    "        if isinstance(data, np.ndarray):\n",
    "            converted = data.copy() if copy else data\n",
    "        elif isinstance(data, pd.Series):\n",
    "            converted = data.values\n",
    "        elif isinstance(data, list):\n",
    "            converted = np.array(data)\n",
    "        elif isinstance(data, pd.DataFrame):\n",
    "            converted = data.values()\n",
    "            \n",
    "    # check if to string is 'list'\n",
    "    elif to == 'list':\n",
    "        \n",
    "        # check for type of list and convert data into useable format for the correlation_ratio ratio \n",
    "        if isinstance(data, list):\n",
    "            converted = data.copy() if copy else data\n",
    "        elif isinstance(data, pd.Series):\n",
    "            converted = data.values.tolist()\n",
    "        elif isinstance(data, np.ndarray):\n",
    "            converted = data.tolist()\n",
    "            \n",
    "    # check if to string is 'dataframe'\n",
    "    elif to == 'dataframe':\n",
    "        \n",
    "        # check for type of dataframe and convert data into useable format for the correlation_ratio ratio \n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            converted = data.copy(deep=True) if copy else data\n",
    "        elif isinstance(data, np.ndarray):\n",
    "            converted = pd.DataFrame(data)\n",
    "            \n",
    "    # raise ValueError\n",
    "    else:\n",
    "        raise ValueError(\"Unknown data conversion: {}\".format(to))\n",
    "        \n",
    "    # check if converted is None and raise TypeError; otherwise return converted\n",
    "    if converted is None:\n",
    "        raise TypeError('cannot handle data conversion of type: {} to {}'.format(type(data), to))\n",
    "    else:\n",
    "        return converted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732d846b-98db-46c6-a7d1-0e465b3404f1",
   "metadata": {},
   "source": [
    "Create function to compute Correlation Ratio.\n",
    "\n",
    "**Sources:** \n",
    "* https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9\n",
    "* https://github.com/shakedzy/dython/blob/master/dython/nominal.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6904f464-2cfb-4bc4-b187-2f65d8f409b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to compute correlation ratio\n",
    "def correlation_ratio(categories: object\n",
    "                      ,measurements: object\n",
    "                      ,nan_strategy: str = 'replace'\n",
    "                      ,nan_replace_value: int = 0.0):\n",
    "    \n",
    "    '''\n",
    "    Calculates the Correlation Ratio (sometimes marked by the greek letter Eta)\n",
    "    for categorical-continuous association.\n",
    "    Answers the question - given a continuous value of a measurement, is it\n",
    "    possible to know which category is it associated with?\n",
    "    Value is in the range [0,1], where 0 means a category cannot be determined\n",
    "    by a continuous measurement, and 1 means a category can be determined with\n",
    "    absolute certainty.\n",
    "    Wikipedia: https://en.wikipedia.org/wiki/Correlation_ratio\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    categories : list / NumPy ndarray / Pandas Series / A sequence of categorical measurements\n",
    "    measurements : list / NumPy ndarray / Pandas Series / A sequence of continuous measurements\n",
    "    nan_strategy : string, default = 'replace'\n",
    "        How to handle missing values: can be either 'drop' to remove samples\n",
    "        with missing values, or 'replace' to replace all missing values with\n",
    "        the nan_replace_value. Missing values are None and np.nan.\n",
    "    nan_replace_value : any, default = 0.0\n",
    "        The value used to replace missing values with. Only applicable when\n",
    "        nan_strategy is set to 'replace'.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float in the range of [0,1]\n",
    "    '''\n",
    "    \n",
    "    # check for null handeling strategy\n",
    "    if nan_strategy == 'replace':\n",
    "        \n",
    "        # run replace nulls function\n",
    "        categories, measurements = replace_nan_with_value(categories, measurements, nan_replace_value)\n",
    "        \n",
    "    elif nan_strategy == 'drop':\n",
    "        \n",
    "        # run remove nulls function\n",
    "        categories, measurements = remove_incomplete_samples(categories, measurements)\n",
    "        \n",
    "    # run convert function\n",
    "    categories = convert(categories, 'array')\n",
    "    measurements = convert(measurements, 'array')\n",
    "    \n",
    "    # factorize the categories\n",
    "    fcat, _ = pd.factorize(categories)\n",
    "    \n",
    "    # calculate the number of categories plus 1\n",
    "    cat_num = np.max(fcat) + 1\n",
    "    \n",
    "    # create an numpy arrays of zeros the length of cat_num\n",
    "    y_avg_array = np.zeros(cat_num)\n",
    "    n_array = np.zeros(cat_num)\n",
    "    \n",
    "    # loop thorugh each category and create the numerator and the denominator needed to compute correlation ratio\n",
    "    for i in range(0, cat_num):\n",
    "        \n",
    "        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n",
    "        n_array[i] = len(cat_measures)\n",
    "        y_avg_array[i] = np.average(cat_measures)\n",
    "        \n",
    "    y_total_avg = np.sum(np.multiply(y_avg_array, n_array)) / np.sum(n_array)\n",
    "    numerator = np.sum(np.multiply(n_array, np.power(np.subtract(y_avg_array, y_total_avg),2)))\n",
    "    denominator = np.sum(np.power(np.subtract(measurements, y_total_avg), 2))\n",
    "    \n",
    "    # check numerator\n",
    "    if numerator == 0:\n",
    "        return 0.\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # compute correlation ratio \n",
    "        eta = np.sqrt(numerator / denominator)\n",
    "        \n",
    "        # check if Theil's U is negative or greater than 1\n",
    "        if 1. < eta <= 1.+1e-13:\n",
    "            \n",
    "            warnings.warn(f'Rounded eta = {eta} to 1. This is probably due to floating point precision issues.', RuntimeWarning)\n",
    "            return 1.\n",
    "        else:\n",
    "            \n",
    "            # return correlation ratio\n",
    "            return eta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ff6e64-83f8-4149-9435-c31034ebf208",
   "metadata": {},
   "source": [
    "## Function to Plot Dates <a id=\"section8\"></a>\n",
    "\n",
    "[Return to Top](#return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066a011c-a723-4d5a-a86b-ba61177255d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to plot dates\n",
    "def plot_dates(data: object\n",
    "              ,print_flag: bool = None\n",
    "              ,file_name_flag: bool = None\n",
    "              ,data_name_var: str = None\n",
    "              ,output_dir: str = None\n",
    "              ):\n",
    "    \n",
    "    '''\n",
    "    Function to plot dates\n",
    "    \n",
    "    param data: pandas dataframe\n",
    "          print_flag: boolean flag; determines whether or not to print output within notebook\n",
    "          file_name_flag: boolean flag; determines whether or not to remove last period and all subsequent characters from a string (i.e. '.csv')\n",
    "          data_name_var: string containing data source name.  typically the file name or database table name\n",
    "          output_dir: string containing output data directory\n",
    "    '''\n",
    "    \n",
    "    # create list of date columns\n",
    "    columns = list(data.select_dtypes(include=['datetime64[ns]']).columns)\n",
    "    \n",
    "    # check if date columns exist\n",
    "    if len(columns) == 0:\n",
    "        print('No dates.')\n",
    "        return\n",
    "    \n",
    "    # check for print flag\n",
    "    if print_flag :\n",
    "        \n",
    "        \n",
    "        # loop through each datetime column and plot distributions\n",
    "        for column in columns:\n",
    "\n",
    "            # group by date and create counts\n",
    "            date_df = data.groupby(column).agg(count = (column, 'count')).sort_values(column).reset_index()\n",
    "\n",
    "            # create date plot\n",
    "            plt.plot_date(date_df[column], date_df['count'], linestyle='solid')\n",
    "                \n",
    "            # format dates and layout\n",
    "            plt.gcf().autofmt_xdate()\n",
    "            date_format = mpl_dates.DateFormatter('%d-%m-%Y')\n",
    "            plt.gca().xaxis.set_major_formatter(date_format)\n",
    "            plt.tight_layout()\n",
    "                \n",
    "            # create plot title and x/y labels\n",
    "            plt.title(f'{column} Distributions')\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Count')\n",
    "            plt.show()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # if there's a period in the data_name_var and if it's a flat file then remove the last period and all subsequent characters (example: '.pkl' or '.csv)\n",
    "        file_title = data_name_var.rpartition('.')[0] if '.' in data_name_var and file_name_flag else data_name_var\n",
    "        \n",
    "        # create pdf object\n",
    "        date_pdf_obj = PdfPages(output_dir + f'05-{file_title}-date_plots_{date.today()}.pdf')\n",
    "\n",
    "        # create plot for each date variable\n",
    "        for column in columns:\n",
    "            \n",
    "            # create figure object\n",
    "            date_fig = plt.figure()\n",
    "            \n",
    "            # group by date and create counts\n",
    "            date_df = data.groupby(column).agg(count = (column, 'count')).sort_values(column).reset_index()\n",
    " \n",
    "            # create date plot\n",
    "            plt.plot_date(date_df[column], date_df['count'], linestyle='solid')\n",
    "                \n",
    "            # format dates and layout\n",
    "            plt.gcf().autofmt_xdate()\n",
    "            date_format = mpl_dates.DateFormatter('%d-%m-%Y')\n",
    "            plt.gca().xaxis.set_major_formatter(date_format)\n",
    "            plt.tight_layout()\n",
    "                \n",
    "            # create plot title and x/y labels\n",
    "            plt.title(f'{column} Distributions')\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Count')\n",
    "            \n",
    "            # save figure\n",
    "            date_pdf_obj.savefig(date_fig, bbox_inches='tight', pad_inches=0.0)\n",
    "            \n",
    "            # close plot\n",
    "            plt.close()\n",
    "            \n",
    "        # close pdf object\n",
    "        date_pdf_obj.close()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
